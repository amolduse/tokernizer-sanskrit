# -*- coding: utf-8 -*-
"""Tokenization Sanskrit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hv2ldKFuEMMYo-pn4AV_35HF-OqKcyWU
"""

from IPython.display import HTML, display
def set_css():
    display(HTML('''
      <style>
        pre{
          white-space: pre-wrap;
        }
      </style>
    '''))

    get_ipython().events.register('pre_run_cell', set_css)

with open('Sanskrit.txt', 'r', encoding='utf-8', errors='replace') as f:
  text = f.read()

print('length: ', len(text))

print(text[:1000])

chars = sorted(list(set(text)))
vocab_size = len(chars)
print(''.join(chars))
print(vocab_size)

stoi = {ch:i for i, ch in enumerate(chars)}
itos = {i:ch for i, ch in enumerate(chars)}

encode = lambda s: [stoi[c] for c in s]
decode = lambda l: ''.join([itos[i] for i in l])

encode('एकवचनम्')
decode([37, 41, 69, 46, 60, 65, 87])

import torch
data = torch.tensor(encode(text), dtype=torch.long)
print(data.shape, data.dtype)
print(data[:10])

ord('अ')
l = list('अ'.encode('utf-8'))
l

tokens = text.encode('utf-8')
tokens = list(map(int, tokens))
tokens
print(len(tokens))

l = [1,2,3,4,5,6,]
#list(zip(l, l[1:]))
l[1:]

def get_stats(ids):
  counts = {}
  for pair in zip(ids, ids[1:]):
    counts[pair] = counts.get(pair, 0) + 1
  return counts

stats = get_stats(tokens)
print(stats)
sorted(((v, k) for k,v in stats.items()), reverse=True)

top_pair = max(stats, key=stats.get)
print(top_pair)

def merge(ids, pair, idx):
  newids = []
  i = 0
  while i < len(ids):
    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:
      newids.append(idx)
      i +=2
    else:
      newids.append(ids[i])
      i += 1
  return newids;

l = [1,2,3,4,5,2,3,7,8,2,3,5,4,2,3]
l = merge(l, [2,3], 256)

tokens2 = merge(tokens, top_pair, 256)
print('length: ', len(tokens2), len(tokens))

vocab_size = 500
num_merges = vocab_size - 256
ids = list(tokens)
merges = {}

for i in range(num_merges):
  stats = get_stats(ids)
  pair = max(stats, key=stats.get)

  idx = 256 + i
  print(f"merging {pair} into a new token {idx}")
  ids = merge(ids, pair, idx)
  merges[pair] = idx

print(merges)

#Fidn the compression ratop
print(f"number of tokens: {len(tokens)}")
print(f"number of ids: {len(ids)}")
print(f"compression ratio: {len(tokens) / len(ids):2f}X")

vocab = {idx: bytes([idx]) for idx in range(256)}
for (p0, p1), idx in merges.items():
  vocab[idx] = vocab[p0] + vocab[p1]

def decode(ids):
  tokens = b"".join(vocab[idx] for idx in ids)
  text = tokens.decode('utf-8', errors="replace")
  return text

decode([65, 257, 305])

def encode(text):
  tokens = text.encode('utf-8')
  while len(tokens) >= 2:
    stats = get_stats(tokens)
    pair = min(stats, key=lambda p: merges.get(p, float("inf")))
    if pair not in merges:
      break # nothing else can be merged
    idx = merges[pair]
    tokens = merge(tokens, pair, idx)
  return tokens

print(encode("एकवचनम्"))

print(encode('एकवचनम्'))
print(decode([256, 370, 401, 273, 258]))

